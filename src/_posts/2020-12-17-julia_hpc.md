---
layout: page 
title: Running Julia jobs on an HPC cluster
shorttitle: Julia on HPC Cluster
permalink: /juliahpc/
tags: julia HPC
---

One of the bigger productivity improvements for me in terms of tweaking and benchmarking algorithms was the ability to run Julia code on my university's HPC nodes. This allowed me to test different versions of my code in parallel while still being able to do other work on my local machine. This is a quick tutorial on how to 
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--4"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-4"
for="sidenote__checkbox--4"
class="sidenote__button sidenote__button--number-4
">setup Julia </label>
<small
id="sidenote-4"
class="sidenote__content sidenote__content--number-4">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
This tutorial is intended for a Julia project, but most of the content applies if you want to run jobs in C++/R/Python/etc.
<span class="sidenote__content-parenthesis">)</span>
</small>
</span> 
on a computing cluster and use SLURM to manage compute jobs.

I am writing this based on my experience with the [University of Oxford HPC  cluster](https://www.arc.ox.ac.uk/arc-systems) (Arcus-HTC) which runs CentOS Linux 7 and uses [SLURM](slurm.schedmd.com/) for job scheduling. This means that based on your university / company setup your experience may differ. In particular the ARC login nodes are connected to the internet which makes installing Julia packages a lot easier. 

## Login and installing Julia
Once you have an account for Oxford ARC, connect to the University VPN and login via ssh:
{% highlight bash %}
ssh -X USERNAME@oscgate.arc.ox.ac.uk
{% endhighlight %}
You'll likely connect to one of the cluster's login nodes. Oxford ARC has different partitions you can choose for your jobs:

- Arcus-B for multi-node parallel computation
- Arcus-HTC for high-throughput lower core count jobs.

I want to use the HTC nodes, so I will again connect to them via ssh:
{% highlight bash %}
ssh -X arcus-htc
{% endhighlight %}

On the login node, two important folders were created for each user:

- `$HOME/` - points to the home directory associated with your user account
- `$DATA/` - a folder to store larger files

To 
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--3"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-3"
for="sidenote__checkbox--3"
class="sidenote__button sidenote__button--number-3
">install Julia </label>
<small
id="sidenote-3"
class="sidenote__content sidenote__content--number-3">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
Some version of Julia/Python/R/Matlab might already be available on ARC. Check with `module avail`.
<span class="sidenote__content-parenthesis">)</span>
</small>
</span> 
we simply download the [Julia binaries](https://julialang.org/downloads/) to our `$DATA/` folder and unzip them:
{% highlight bash %}
cd $DATA
wget https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz
unzip julia-1.5.3-linux-x86_64.tar.gz
{% endhighlight %}

To run Julia on the login node we can type:
{% highlight bash %}
./julia-1.5.3-linux-x86_64/bin/julia
{% endhighlight bash %}
(You should generally not run any significant computations on the login nodes, but it is a good way to check that everything works. Also it is a good idea to create a symlink or alias to `/julia-1.5.3-linux-x86_64/bin/julia`. )


## Installing Julia packages
You'll find your Julia home folder at `$HOME/.julia/`. Since the ARC nodes are connected to the internet you can simply download any packages using the normal workflow. I am assuming here that we want to setup some scripts in the project folder `$DATA/myproject/`:

    1. Run Julia
    2. Create a new project environment `activate .`
    3. Install packages: Type `]` and `pkg> add Statistics`

## Uploading your code
This is a matter of taste but I prefer to be able to modify any of my code both locally and remotely. I therefore initialise `/myproject` as a
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--1"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-1"
for="sidenote__checkbox--1"
class="sidenote__button sidenote__button--number-1
">Git repository</label>.
<small
id="sidenote-1"
class="sidenote__content sidenote__content--number-1">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
A quick overview to get started with Git can be found [here](https://rogerdudler.github.io/git-guide/). 
<span class="sidenote__content-parenthesis">)</span>
</small>
</span>
I can then edit a local copy and `git push` any of the local changes or vice versa. The project dependencies for this project are tracked by Julia in the  `Project.toml` and `Manifest.toml` files. Changes in both files are also tracked via Git to make sure the same dependencies are used locally and remotely.

This setup allows me to test or debug any changes to my code locally and be certain that it will run the same way on the remote node. Debugging the code on the remote node is more time consuming because your compute jobs do not necessarily execute immediately. For example it might take several minutes until you receive the error message that you misspelled a function name.

Sometimes I want to upload files from my machine without tracking them via git, e.g. large dataset files like `dataset.csv`. To transfer files I use `scp` 
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--5"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-5"
for="sidenote__checkbox--5"
class="sidenote__button sidenote__button--number-5
">(secure copy protocol)</label>.
<small
id="sidenote-5"
class="sidenote__content sidenote__content--number-5">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
Syntax: scp [OPTION] [user@SRC_HOST:]file1 [user@DEST_HOST:]file2
<span class="sidenote__content-parenthesis">)</span>
</small>
</span>
To upload `dataset.csv` I simply type:
{% highlight bash %}
scp dataset.csv USERNAME@oscgate.arc.ox.ac.uk:/home/USERNAME/.
{% endhighlight bash %}
To transfer a result file `results.csv` back to the current folder on my local machine I can use the same command:
{% highlight bash %}
scp USERNAME@oscgate.arc.ox.ac.uk:/home/USERNAME/results.csv .
{% endhighlight bash %}

## Scheduling jobs
Oxford ARC uses the [SLURM](slurm.schedmd.com/) workload manager to request and manage compute jobs. Assuming we are still in the folder `$DATA/myproject` the basic workflow to create a new job is:

- write a Julia script `myscript.jl` to run your code
- manage all Julia project dependencies using the environment files `Project.toml` and `Manifest.toml`
- write a job submission script `run_job.sh` that requests compute resources and tells the compute nodes what to do

Let's assume we want to run the following script `myscript.jl` to (inefficiently) compute and print the 20-th Fibonacci number:
{% highlight julia %}
function fibonacci(n::Int64)
    if n <= 1
        return n
    else
        return fibonacci(n - 1) + fibonacci(n - 2)
    end
end

println("The 20-th fibonacci number is $(fibonacci(20)).")
{% endhighlight %}

To run this script on one of the compute nodes we define a submission script `run_job.sh`: 
{% highlight bash%}
#!/bin/bash

#SBATCH --time=0:05:00
#SBATCH --job-name="Fibonacci_calculation"
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --partition=htc
#SBATCH --output="Fibonacci.out"
#SBATCH --error="Fibonacci.err.out"
#SBATCH --mail-type=ALL
#SBATCH --mail-user=YOUR_EMAIL_ADDRESS

$DATA/julia-1.5.3-linux-x86_64/bin/julia --project -e 'import Pkg; Pkg.instantiate();
include("myscript.jl")'
{% endhighlight bash %}
Lines starting with `#SBATCH` are SLURM commands. We request 5min of computation time on one CPU. We further want to run it on the ARC-HTC partition and get status updates about our job sent to our email.

The last line in the script calls Julia, instantiates the 
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--7"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-7"
for="sidenote__checkbox--7"
class="sidenote__button sidenote__button--number-7
">project environment</label>,
<small
id="sidenote-7"
class="sidenote__content sidenote__content--number-7">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
Suggested reading: [Working with environments](https://julialang.github.io/Pkg.jl/v1/environments/).
<span class="sidenote__content-parenthesis">)</span>
</small>
</span>i.e. installs any package dependencies defined in `Project.toml`, and runs our script.

We then submit the job using the shell with
{% highlight bash%}
sbatch run_job.sh
{% endhighlight bash %}

Depending on the available resources the job is then queued and waiting for execution. You can look up the current job status with
{% highlight bash%}
squeue -u USERNAME
{% endhighlight bash %}
To cancel the job use either of the two:
{% highlight bash%}
scancel -j [JOBNUMBER]
scancel -u [USERNAME]
{% endhighlight bash %}

### Job arrays
Sometimes we want to run the same job multiple times with only minor modifications, e.g. running our algorithm with one of the hyperparameters changed. For this case SLURM [job arrays](https://slurm.schedmd.com/job_array.html) are quite useful. They execute the submission script multiple times and allow you to run different versions of the same script.
The ability to run time-consuming jobs in parallel can be a big time saver.
Let's assume that we want to calculate the Fibonacci number for `n = 20, 30, 40` in separate jobs. 

To achieve this we add one line with an array command to our submission script `run_job.sh`:
{% highlight bash%}
#!/bin/bash

#SBATCH --time=0:05:00
#SBATCH --job-name="Fibonacci_calculation"
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --partition=htc
#SBATCH --output="Fibonacci_%a.out"
#SBATCH --error="Fibonacci_%a.err.out"
#SBATCH --mail-type=ALL
#SBATCH --mail-user=YOUR_EMAIL_ADDRESS
#SBATCH --array=1-3

$DATA/julia-1.5.3-linux-x86_64/bin/julia --project -e 'import Pkg; Pkg.instantiate();
include("myscript.jl")'
{% endhighlight bash %}
The job will now run three times. The array id is given by the environment variable`SLURM_ARRAY_TASK_ID` which we can use inside our Julia script to select `n`:
{% highlight julia %}
# get the environment variable
task_id = Base.parse(Int, ENV["SLURM_ARRAY_TASK_ID"])
n_arr = [20; 30; 40]
n = n_arr[task_id]

function fibonacci(n::Int64)
    if n <= 1
        return n
    else
        return fibonacci(n - 1) + fibonacci(n - 2)
    end
end

println("The $(n)-th fibonacci number is $(fibonacci(n)).")
{% endhighlight %}


### Multithreading
To use multithreading in our Julia script we have to request multiple cores on a computing node. The following script `my_multithreaded_script.jl` is used to compute the Fibonacci number for different `n` on
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--6"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-6"
for="sidenote__checkbox--6"
class="sidenote__button sidenote__button--number-6
">parallel threads
</label>:
<small
id="sidenote-6"
class="sidenote__content sidenote__content--number-6">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
This is not the best example for multithreading, because computing the Fibonacci number for the highest `n` computes the Fibonacci number for all lower `n` in the process.
<span class="sidenote__content-parenthesis">)</span>
</small>
</span>

{% highlight julia %}
# check number of threads
println("Number of threads: $(Threads.nthreads())")

n_arr = [5; 10; 15; 20; 25; 30; 35; 40]

function fibonacci(n::Int64)
    if n <= 1
        return n
    else
        return fibonacci(n - 1) + fibonacci(n - 2)
    end
end

# use multiple threads to compute the fibonacci number for each n in n_arr
fib = zeros(length(n_arr))
Threads.@threads for k = 1:length(n_arr)
    fib[k] = fibonacci(n_arr[k])
end
{% endhighlight %}

To run this script we use the following modified SLURM submission script:
{% highlight bash%}
#!/bin/bash

#SBATCH --time=0:05:00
#SBATCH --job-name="Fibonacci_multithreaded_calculation"
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --partition=htc
#SBATCH --output="Fibonacci.out"
#SBATCH --error="Fibonacci.err.out"
#SBATCH --mail-type=ALL
#SBATCH --mail-user=YOUR_EMAIL_ADDRESS

export JULIA_NUM_THREADS=8
$DATA/julia-1.5.3-linux-x86_64/bin/julia --project -e 'import Pkg; Pkg.instantiate();
include("my_multithreaded_script.jl")'
{% endhighlight bash %}
The only changes are that we now request 8 cores on one node and export the environment variable `JULIA_NUM_THREADS` to start Julia with 8 threads.


### Ensuring consistent results
Let's assume that you want to benchmark your algorithm, e.g. measure its execution time. In order to generate consistent results from multiple runs, you have to make sure that the Julia script is executed on the same hardware every time. If no hardware is specified SLURM will just run the job on the next available node. If the node has eight CPUs and your job runs on four of them, then the performance of your job will depend on what other jobs run on the remaining four CPUs.

To ensure
<span class="sidenote">
<input
aria-label="Show sidenote"
type="checkbox"
id="sidenote__checkbox--2"
class="sidenote__checkbox">
<label
tabindex="0"
title="{{ include.content }}"
aria-describedby="sidenote-2"
for="sidenote__checkbox--2"
class="sidenote__button sidenote__button--number-2
">consistent results</label>
<small
id="sidenote-1"
class="sidenote__content sidenote__content--number-2">
<span class="sidenote__content-parenthesis
"> (sidenote: </span>
There will still be some variance in your time measurement. If more accuracy is required, I suggest running the script a number of times (using a job array) and averaging the result.
<span class="sidenote__content-parenthesis">)</span>
</small>
</span> you will have to specify the node hardware and request all the CPUs on a node. Information about the different nodes within the Oxford HTC partition can be found [here](https://www.arc.ox.ac.uk/arcus-htc-reference-guide).

Let's assume that we want to run our job on a SandyBridge E5-2650 (2GHz) node. We can request exclusive access to a whole node by using SLURM constraints. Just add the following lines to the `run_job.sh` submission script:
{% highlight bash%}
#SBATCH --constraint='cpu_sku:E5-2650'
#SBATCH --exclusive
{% endhighlight bash %}






